{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextBlob is already installed but we need data files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now import some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sqlite\n",
    "import pandas as pd\n",
    "import os\n",
    "from textblob import TextBlob, Word\n",
    "from ipywidgets import widgets, interact, interactive, fixed\n",
    "\n",
    "from IPython.display import clear_output, display, HTML\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data is stored in a [SQLite](https://docs.python.org/2/library/sqlite3.html) database. Use Pandas to read it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = os.path.join(\"..\",\"Resources\")\n",
    "con = sqlite.connect(os.path.join(DATADIR,\"reports.sqlite\"))\n",
    "df = pd.read_sql(\"SELECT * from reports\", con)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can rename a column to make it more friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'0':'report'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATADIR,\"atlantic_article.txt\")) as f0:\n",
    "    article = f0.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML( article.replace('\\n',\"</br>\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a TextBlob Object\n",
    "\n",
    "When we create a ``TextBlob`` object it does a lot of behind the scence processing for us, such as\n",
    "\n",
    "* Breaking the text into sentences\n",
    "* Breaking the text into words\n",
    "* Breaking the text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob can tokenize (break into pieces) the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in blob.sentences[:5]:\n",
    "    print (s)\n",
    "    print(\"-\"*42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in blob.words[:40]:\n",
    "    print (w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens include punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in blob.tokens[:40]:\n",
    "    print (t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sentiment Analysis](http://en.wikipedia.org/wiki/Sentiment_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Generally speaking, sentiment analysis aims to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document. The attitude may be his or her judgment or evaluation (see appraisal theory), affective state (that is to say, the emotional state of the author when writing), or the intended emotional communication (that is to say, the emotional effect the author wishes to have on the reader). (Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level â€” whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral. (Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Another research direction is subjectivity/objectivity identification. This task is commonly[8] defined as classifying a given text (usually a sentence) into one of two classes: objective or subjective. (Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob does Some Sentiment Analysis\n",
    "#### Sentiment can be computed on a document and sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in blob.sentences:\n",
    "    print (s)\n",
    "    print (s.sentiment)\n",
    "    print ('-'*42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing Words\n",
    "\n",
    "* We often want to transform all the variations of words into a single form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextBlob Can Singularize and Pluralize Words, But Not Well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = blob.sentences[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in s.words:\n",
    "    print (w,w.singularize(),w.pluralize())\n",
    "    print(\"-\"*42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextBlob Can Lemmatize Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Lemmatisation (or lemmatization) in linguistics is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.[1] (Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = blob.sentences[15]\n",
    "for w in s.words:\n",
    "    if w != w.lemmatize():\n",
    "        print (\"Modified:\\t\",w,\"->\",w.lemmatize())\n",
    "    else:\n",
    "        print(\"Unchanged:\\t\",w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can guide the lemmatization by telling word type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww = Word(\"imagining\")\n",
    "print(ww.lemmatize('v'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob will do part-of-speech tagging\n",
    "\n",
    "A list of part-of-speech tag abbreviations can be seen [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(blob.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = dict(blob.tags)\n",
    "for item in list(t.items())[0:20]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Can (try) to Provide Definitions\n",
    "\n",
    "If we do not provide a part-of-speech, TextBlob will try to return definitions for all parts-of-speech. Definitions are returned as a [synset](https://en.wikipedia.org/wiki/Synonym_ring)\n",
    "\n",
    ">In metadata a synonym ring or synset, is a group of data elements that are considered semantically equivalent for the purposes of information retrieval. These data elements are frequently found in different metadata registries. Although a group of terms can be considered equivalent, metadata registries store the synonyms at a central location called the preferred data element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in s.words:\n",
    "    print (w)\n",
    "    for d in w.definitions:\n",
    "        print(\"*\",d)\n",
    "    print ('-'*42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Word('apical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w1.definitions)\n",
    "w1.lemmatize('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.words.count(\"eggs\",case_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.words.count(\"eggs\",case_sensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob can try to translate\n",
    "#### NOTE: Google has eliminated their free translation service\n",
    "\n",
    "* Translation is done via Google Translate\n",
    "\n",
    "[Language Codes Can be Found Here](https://cloud.google.com/translate/v2/using_rest#language-params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = [l.split(\"\\t\") for l in \"\"\"Afrikaans \taf\n",
    "Albanian \tsq\n",
    "Arabic \tar\n",
    "Armenian \thy\n",
    "Azerbaijani \taz\n",
    "Basque \teu\n",
    "Belarusian \tbe\n",
    "Bengali \tbn\n",
    "Bosnian \tbs\n",
    "Bulgarian \tbg\n",
    "Catalan \tca\n",
    "Cebuano \tceb\n",
    "Chichewa \tny\n",
    "Chinese Simplified \tzh-CN\n",
    "Chinese Traditional \tzh-TW\n",
    "Croatian \thr\n",
    "Czech \tcs\n",
    "Danish \tda\n",
    "Dutch \tnl\n",
    "English \ten\n",
    "Esperanto \teo\n",
    "Estonian \tet\n",
    "Filipino \ttl\n",
    "Finnish \tfi\n",
    "French \tfr\n",
    "Galician \tgl\n",
    "Georgian \tka\n",
    "German \tde\n",
    "Greek \tel\n",
    "Gujarati \tgu\n",
    "Haitian Creole \tht\n",
    "Hausa \tha\n",
    "Hebrew \tiw\n",
    "Hindi \thi\n",
    "Hmong \thmn\n",
    "Hungarian \thu\n",
    "Icelandic \tis\n",
    "Igbo \tig\n",
    "Indonesian \tid\n",
    "Irish \tga\n",
    "Italian \tit\n",
    "Japanese \tja\n",
    "Javanese \tjw\n",
    "Kannada \tkn\n",
    "Kazakh \tkk\n",
    "Khmer \tkm\n",
    "Korean \tko\n",
    "Lao \tlo\n",
    "Latin \tla\n",
    "Latvian \tlv\n",
    "Lithuanian \tlt\n",
    "Macedonian \tmk\n",
    "Malagasy \tmg\n",
    "Malay \tms\n",
    "Malayalam \tml\n",
    "Maltese \tmt\n",
    "Maori \tmi\n",
    "Marathi \tmr\n",
    "Mongolian \tmn\n",
    "Myanmar (Burmese) \tmy\n",
    "Nepali \tne\n",
    "Norwegian \tno\n",
    "Persian \tfa\n",
    "Polish \tpl\n",
    "Portuguese \tpt\n",
    "Punjabi \tma\n",
    "Romanian \tro\n",
    "Russian \tru\n",
    "Serbian \tsr\n",
    "Sesotho \tst\n",
    "Sinhala \tsi\n",
    "Slovak \tsk\n",
    "Slovenian \tsl\n",
    "Somali \tso\n",
    "Spanish \tes\n",
    "Sudanese \tsu\n",
    "Swahili \tsw\n",
    "Swedish \tsv\n",
    "Tajik \ttg\n",
    "Tamil \tta\n",
    "Telugu \tte\n",
    "Thai \tth\n",
    "Turkish \ttr\n",
    "Ukrainian \tuk\n",
    "Urdu \tur\n",
    "Uzbek \tuz\n",
    "Vietnamese \tvi\n",
    "Welsh \tcy\n",
    "Yiddish \tyi\n",
    "Yoruba \tyo\n",
    "Zulu \tzu\"\"\".split(\"\\n\")]\n",
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = blob.sentences[0]\n",
    "type(s)\n",
    "type(s.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(s=[s.raw for s in blob.sentences],code={c[0]:c[1] for c in codes})\n",
    "def translate_sentence(s,code):\n",
    "    #clear_output()\n",
    "    blob = TextBlob(s)\n",
    "    try:\n",
    "        display(HTML(\"<h3>Original</h3><p>%s</p><h2>To: %s</h2>\"%(s,code[0])))\n",
    "        display(HTML(blob.sentences[0].translate(to=code).raw))\n",
    "    except Exception as error:\n",
    "        display(HTML(\"<h3>Could not translate: %s</h3>\"%error))\n",
    "\n",
    "    \n",
    "\n",
    "seq=\" \".join(list(\"\"\"AACGAACGCTGGCGGCATGCCTAACACATGCAAGTCGAACGAGACCTTCGGGTCTAGTGGCGCACGGGTGCGTAACGCGTGGGAA\"\"\"\\\n",
    "    +\"\"\"TCTGCCCTTGGGTACGGAATAACAGTTAGAAATGACTGCTAATACC\"\"\"))\n",
    "sblob = TextBlob(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob can create n-grams (think k-mers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in blob.sentences:\n",
    "    print( len(s.ngrams(2)),len(s.ngrams(3)),len(s.ngrams(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq=\" \".join(list(\"\"\"AACGAACGCTGGCGGCATGCCTAACACATGCAAGTCGAACGAGACCTTCGGGTCTAGTGGCGCACGGGTGCGTAACGCGTGGGAA\"\"\"\\\n",
    "    +\"\"\"TCTGCCCTTGGGTACGGAATAACAGTTAGAAATGACTGCTAATACC\"\"\"))\n",
    "sblob = TextBlob(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sblob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams=sblob.ngrams(7)\n",
    "ngrams.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngs = [''.join(n) for n in ngrams]\n",
    "print (len(ngs))\n",
    "for n in ngs:\n",
    "    print (n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
